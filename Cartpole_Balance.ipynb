{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This notebook is a simple solution to the cartpole balancing game using 2 methods\n",
    "\n",
    "## 1) Random Search:\n",
    "This method simply creates many random agents and chooses the best one. Because our problem is very simple, this method works well enough. If the problem is more complex, it will need more parameters that define its agents; this would, therefore, make random search not efficient as most of the random agents will not be solving our problem well enough.\n",
    "\n",
    "## 2) Random ascent:\n",
    "This method is also very simple, but it still has the dimensionality problem (complex problems are hard to solve using this method). This method works by starting with a random agent (you could even run random search on a number of agents and use the best one to start the random ascent). Then, you pick random numbers to add to your current agent's parameters. If the agent improves, keep the change; otherwise, neglect it.\n",
    "\n",
    "### The cartpole balancing game\n",
    "I used the OpenAI's gym environment to run the game. The game is very simple. The player controls a cart that he/she can move to the left or to the right. The player should try to balance a pole that is standing vertically on the cart without the pole going out of control and falling.\n",
    "<img src=\"img/cartpole_game.png\">\n",
    "The game is considered solved if, according to OpenAI's documentation, \"when the average reward is greater than or equal to 195.0 over 100 consecutive trials\". I will try to accomplish it in the least time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing dependencies\n",
    "import numpy as np\n",
    "import gym\n",
    "import time\n",
    "\n",
    "env = gym.make('CartPole-v0') # Creating the openAI environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "This function run an agent in an environment until the episode (game) terminates.\n",
    "\n",
    "Args:\n",
    "    env: OpenAI's env object\n",
    "    parameters: numpy array of paramters that describe the agent\n",
    "    \n",
    "Returns:\n",
    "    The total reward after the episode terminates\n",
    "'''\n",
    "def run_episode(env,parameters):\n",
    "    observation = env.reset() # Get initial observations when the evironment starts\n",
    "    total_reward = 0\n",
    "    for _ in range(200):\n",
    "        action = 0 if np.matmul(parameters,observation) < 0 else 1 # The agents decision is a simple linear combination of observations and paramters\n",
    "        observation, reward, done, info = env.step(action)\n",
    "        total_reward += reward\n",
    "        if done:\n",
    "            break\n",
    "    return total_reward\n",
    "\n",
    "'''\n",
    "Runs n episodes (using run_episode) to try to minimize variance in the measurement of rewards\n",
    "Args:\n",
    "    n: number of episodes to run\n",
    "    env: OpenAI's env object\n",
    "    parameters: numpy array of paramters that describe the agent\n",
    "\n",
    "Returns:\n",
    "    The total reward averaged over n episodes\n",
    "'''\n",
    "def run_episodes(n,env,parameters):\n",
    "    total_reward = 0\n",
    "    for _ in range(n):\n",
    "        total_reward += run_episode(env,parameters)\n",
    "    return total_reward/n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First we will try to use random search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Execution Time: 28.429315090179443\n",
      "Average agent fitness: 197.72530000000003\n",
      "Finding best agent takes 24.83 iterations, each ran the game for 4 episodes => ran the game on average 99.32 times.\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Runs random search to find the best agent\n",
    "Args:\n",
    "    num_episodes: The number of episodes used in run_episodes to estimate the fitness of an agent\n",
    "    max_reward: The maximum reward an agent can achieve\n",
    "    print_info: If True, info about the best agent and its training is printed\n",
    "\n",
    "Returns:\n",
    "    best_agent: Parameters of the best agent found\n",
    "    best_agent_eval: The fitness of the best agent after running it for 100 episodes (The initial goal was to have a fitness > 195 for 100 episodes)\n",
    "    running_time: The running time of the random search algorithm\n",
    "    num_iterations: The number of random agents tested before the best one is found\n",
    "'''\n",
    "def random_search(num_episodes=8,max_reward=200.0,print_info=False):\n",
    "    best_parameters = None\n",
    "    best_reward = 0\n",
    "    num_iterations = 0\n",
    "    initial_time = time.time()\n",
    "    for _ in range(10000):\n",
    "        num_iterations += 1\n",
    "        parameters = np.random.rand(4)*2 - 1 # Random agent\n",
    "        reward = run_episodes(num_episodes,env,parameters) # A measure of quality of the agent\n",
    "        if reward>best_reward:\n",
    "            best_reward = reward\n",
    "            best_parameters = parameters\n",
    "        if best_reward == max_reward:\n",
    "            break\n",
    "    end_time = time.time()\n",
    "    running_time = (end_time-initial_time)*1000\n",
    "    best_agent_eval = run_episodes(100,env,best_parameters)\n",
    "    \n",
    "    if print_info:\n",
    "        print(\"Time of execution: {} ms\".format(running_time))\n",
    "        print(\"Found best agent after {} iterations, each ran the game for {} episodes => ran the game {} times.\".format(num_iterations,num_episodes,num_iterations*num_episodes))\n",
    "        print(\"Agent Evaluation: {}\".format(best_agent_eval))\n",
    "    \n",
    "    return best_parameters,best_agent_eval,running_time,num_iterations\n",
    "\n",
    "num_episodes = 4 # Seems to be the smallest number to achieve an average total reward of at least 195\n",
    "num_tests = 100\n",
    "\n",
    "avg_fitness = 0\n",
    "avg_run_time = 0\n",
    "avg_iters = 0\n",
    "for _ in range(num_tests):\n",
    "    _,fitness,run_time,iters = random_search(num_episodes)\n",
    "    avg_fitness += fitness\n",
    "    avg_run_time += run_time\n",
    "    avg_iters += iters\n",
    "\n",
    "avg_fitness /= num_tests\n",
    "avg_run_time /= num_tests\n",
    "avg_iters /= num_tests\n",
    "\n",
    "print(\"Average Execution Time: {}\".format(avg_run_time))\n",
    "print(\"Average agent fitness: {}\".format(avg_fitness))\n",
    "print(\"Finding best agent takes {} iterations, each ran the game for {} episodes => ran the game on average {} times.\".format(avg_iters,num_episodes,avg_iters*num_episodes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next, We will try random ascent to see the difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.2\n",
      "Time of execution: 6060.2171421051025 ms\n"
     ]
    }
   ],
   "source": [
    "noise_scaling = 0.1 # The amount of randomness to try to add at each iteration\n",
    "best_parameters_ascent = (np.random.rand(4)*2 - 1) # Initial random agent\n",
    "best_reward = 0\n",
    "max_reward = 200.0\n",
    "initial_time = time.time()\n",
    "for _ in range(10000):\n",
    "    parameters = best_parameters_ascent + (np.random.rand(4)*2 - 1)*noise_scaling # Add random numbers to the current agent\n",
    "    reward = run_episodes(10,env,parameters)\n",
    "    if reward>best_reward: # If the agent improved, keep it; otherwise, neglect it.\n",
    "        best_reward = reward\n",
    "        best_parameters_ascent = parameters\n",
    "    if best_reward == max_reward:\n",
    "        break\n",
    "\n",
    "end_time = time.time()\n",
    "print(best_reward)\n",
    "print(\"Time of execution: {} ms\".format((end_time-initial_time)*1000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running the best agent on the game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "observation = env.reset()\n",
    "timestep = 0\n",
    "for _ in range(200):\n",
    "    best_action = 0 if np.matmul(best_parameters_ascent,observation) < 0 else 1\n",
    "    observation, reward, done, info = env.step(best_action)\n",
    "    env.render()\n",
    "    if done:\n",
    "        break\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
